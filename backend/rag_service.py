from dotenv import load_dotenv
load_dotenv()

# Components
from langchain.chat_models import init_chat_model
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma

llm = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

vector_store = Chroma(
    collection_name="plm_assistant_db",
    embedding_function=embeddings,
    persist_directory="./admin/data/chroma_plm_db",
)

# Retrieval and Generation

from langchain_core.prompts import ChatPromptTemplate

# N.B. for non-US LangSmith endpoints, you may need to specify
# api_url="https://api.smith.langchain.com" in hub.pull.

prompt = ChatPromptTemplate.from_template("""
    You are a helpful assistant. Use the following context to answer the question.
    If the answer cannot be found, say "I’m not sure based on the provided context."

    Context:
    {context}

    Question:
    {question}

    Answer:
    """)

example_messages = prompt.format_messages(
    context="(context goes here)",
    question="(question goes here)"
)
print(example_messages[0].content)


from langchain_core.documents import Document
from typing_extensions import List, TypedDict

# The state of our application controls what data is input to the application, 
# transferred between steps, and output by the application. 
# It is typically a TypedDict, but can also be a Pydantic BaseModel.
# With TypedDict you don’t instantiate — you just create a dict that conforms to the TypedDict shape.
class State(TypedDict):
    question: str           # The user’s question (string)
    context: List[Document] # A list of retrieved documents (from Pinecone / vector store)
    answer: str             # The answer generated by your model

def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}

def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


# Control Flow
from langgraph.graph import START, StateGraph

graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()


"""

# Invoke
result = graph.invoke({"question": "Can you tell me all the Rules I have to Know for the 1st day of class at PLM?"})

print(f"Context: {result['context']}\n\n")
print(f"Answer: {result['answer']}")

"""