from dotenv import load_dotenv
load_dotenv()

# Components
from langchain.chat_models import init_chat_model
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma

llm = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

vector_store = Chroma(
    collection_name="plm_assistant_db",
    embedding_function=embeddings,
    persist_directory="./admin/data/chroma_plm_db",
)


from langchain_core.documents import Document
from typing_extensions import List, TypedDict
from typing_extensions import Annotated
from typing import Literal

# The state of our application controls what data is input to the application, transferred between steps, and output by the application. 
# It is typically a TypedDict, you don’t instantiate — you just create a dict that conforms to the TypedDict shape. Just for the compiler to recognize.
class Search(TypedDict):
    """ This search class is used for Query Analysis where we analyze the class of the question """
    # this query is redundant and I didnt used this. I sticked witht he original question
    query: Annotated[str, ..., "Search query to run."]

    # Annotated means: section must be one of these 8 exact strings, and it’s required, and here’s a description.
    section: Annotated[
        Literal['general_university_information', 
                'student_rights_and_responsibility', 
                'academic_policies', 
                'student_affairs_and_services',
                'student_councils_organizations_and_activities_policies',
                'campus_publication',
                'disciplinary_policies',
                'general_provisions_glossary_and_appendices'],
        ...,
        "Section to query.",
    ]

class State(TypedDict):
    question: str           # The user’s question (string)
    query: Search           # structured question version (with section property)
    hypothetical_question: str
    context: List[Document] # A list of retrieved documents (from Pinecone / vector store)
    answer: str             # The answer generated by your model


# Query analysis
def analyze_query(state: State):
    """ The state would have query which has dict of query and section
        I didn't use the query but only the state['query']['section'] part later for filtering.
        The state['query']['query'] part is kinda redundant cause I use the original question throughout and they just the same.
    """
    print('started analyzing query...')

    structured_llm = llm.with_structured_output(Search)
    query = structured_llm.invoke(state["question"])
    print('analyzed query...')
    return {'query': query}


# Hypothetical Document Embeddings
from langchain_core.prompts import ChatPromptTemplate
hyde_prompt = ChatPromptTemplate.from_template("""
    User question:
    {question}

    Instructions:
    - If the question is about PLM university policies, academic rules, disciplinary procedures (e.g., Latin honors, dropping subjects), factual university information (e.g., president, offices), or urgent student safety matters (e.g., bullying, harassment, assault), generate a short (2–3 sentence) *hypothetical handbook-style answer* that plausibly appears in the PLM Student Manual.
    - If the input is just a greeting, small talk, or unrelated, simply repeat the user’s input as-is (do not generate a hypothetical policy document).
    - Use a formal tone only when generating a policy answer.
    """)


def generate_hypothetical(state: State): 
    print('generating hypothetical prompt...')
    messages = hyde_prompt.invoke({"question": state["question"]})
    response = llm.invoke(messages)
    print('done generated hypothetical prompt')

    return {"hypothetical_question": response.content}


# Retrieval and Generation
prompt = ChatPromptTemplate.from_template("""
    You are a friendly assistant specializing in Pamantasan ng Lungsod ng Maynila (PLM) rules and regulations. Answer the user's question using ONLY the provided context.

    Instructions:
    1. Read and synthesize ALL relevant context chunks.
    2. Summarize the policy clearly, using formal terms (e.g., GWA, Maximum Residency Rule, OSDS) to ensure accuracy.
    3. Integrate related policies or sections when needed.
    4. If the question is NOT covered in the context, respond politely stating you do not have knowledge outside the information you are given.
    5. Keep a conversational and engaging tone; greet the user if they start with "hi" or "hello."
    6. Use the same language as the user's input when generating the answer.
    7. When referring to your source, say “based on the information I have” instead of “context.”

    Context:
    {context}

    Question:
    {question}
    """)

def retrieve(state: State):
    hyde_text = state["hypothetical_question"] or state["question"]
    query = state["query"]
    retrieved_docs = vector_store.similarity_search(hyde_text, k=8, filter={'section': query['section']})
    print('done retrieve...')

    return {"context": retrieved_docs}

def generate(state: State):
    print('started generate...')

    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content}) # takes a dictionary of input variables and fills them into the template.
    print('got context and putted in prompt message...')

    response = llm.invoke(messages)
    print('done generate...')

    return {"answer": response.content}

# Control Flow
from langgraph.graph import START, StateGraph

graph_builder = StateGraph(State).add_sequence([analyze_query, generate_hypothetical, retrieve, generate])
graph_builder.add_edge(START, "analyze_query")
graph = graph_builder.compile()