from dotenv import load_dotenv
load_dotenv()

# Components
from langchain.chat_models import init_chat_model
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma

llm = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

vector_store = Chroma(
    collection_name="plm_assistant_db",
    embedding_function=embeddings,
    persist_directory="./admin/data/chroma_plm_db",
)

# Retrieval and Generation
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("""
    You are a helpful assistant. You are given multiple pieces of context (chunks). 

    Your task:
    1. Find the chunk(s) that best answer the user’s question.
    2. Answer based ONLY on the chunks provided.
    3. If the answer cannot be found, say "I’m not sure based on the provided context."

    Context:
    {context}

    Question:
    {question}

    Answer:
    """)

from langchain_core.documents import Document
from typing_extensions import List, TypedDict

# The state of our application controls what data is input to the application, 
# transferred between steps, and output by the application. 
# It is typically a TypedDict, but can also be a Pydantic BaseModel.
# With TypedDict you don’t instantiate — you just create a dict that conforms to the TypedDict shape.
class State(TypedDict):
    question: str           # The user’s question (string)
    context: List[Document] # A list of retrieved documents (from Pinecone / vector store)
    answer: str             # The answer generated by your model

def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"], k=5)
    return {"context": retrieved_docs}

def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


# Control Flow
from langgraph.graph import START, StateGraph

graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()


"""

# Invoke
result = graph.invoke({"question": "Can you tell me all the Rules I have to Know for the 1st day of class at PLM?"})

print(f"Context: {result['context']}\n\n")
print(f"Answer: {result['answer']}")

"""